{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c165297b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from scipy.stats import binned_statistic_2d\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc426a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "period_full = pd.date_range(datetime(2016,1,1), datetime(2016,6,30,23), freq='H')\n",
    "period_train = pd.date_range(datetime(2016,1,1), datetime(2016,4,30,17), freq='H')\n",
    "period_train2 = pd.date_range(datetime(2016,1,1), datetime(2016,5,31,17), freq='H')\n",
    "period_validation = pd.date_range(datetime(2016,4,30,23), datetime(2016,5,31,17), freq='H')\n",
    "period_test = pd.date_range(datetime(2016,5,31,23), datetime(2016,6,30,17), freq='H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9751f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# праздники\n",
    "holidays = pd.read_csv('usa holidays.csv', header=None, parse_dates=[0])\n",
    "holidays.columns = ['date']\n",
    "holidays = holidays.set_index('date')\n",
    "holidays['holiday'] = 1\n",
    "\n",
    "# регионы\n",
    "regions = [int(reg) for reg in pd.read_csv('regions.csv').columns.values]\n",
    "len(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f62dea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# координаты Нью-Йорка\n",
    "nyLongFrom, nyLongTo = -74.25559, -73.70001\n",
    "nyLatFrom, nyLatTo = 40.49612, 40.91553\n",
    "\n",
    "\n",
    "\n",
    "# очищаем данные\n",
    "def clear_data(data):\n",
    "    # в сырых данных реестр колонок иногда меняется, приведем все имена колонок в нижний реестр\n",
    "    data.columns = [str(col).lower() for col in data.columns]\n",
    "    \n",
    "    to_drop_indicies = data[\n",
    "        (data['pickup_latitude'] < nyLatFrom) |\n",
    "        (data['pickup_latitude'] > nyLatTo) |\n",
    "        (data['pickup_longitude'] < nyLongFrom) |\n",
    "        (data['pickup_longitude'] > nyLongTo) |\n",
    "        (data['passenger_count'] == 0) |\n",
    "        (data['trip_distance'] == 0) |\n",
    "        (data['tpep_dropoff_datetime'] <= data['tpep_pickup_datetime']) ].index\n",
    "    data = data.drop(to_drop_indicies, axis=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# добавляем регионы начала и окончания пути\n",
    "def add_region(data):\n",
    "    _, _, _, binnum_pickup = binned_statistic_2d(data['pickup_longitude'].to_numpy(),\n",
    "                                                 data['pickup_latitude'].to_numpy(),\n",
    "                                                 values = None, statistic = 'count',\n",
    "                                                 bins = [50, 50],\n",
    "                                                 range = [[nyLongFrom, nyLongTo], [nyLatFrom, nyLatTo]],\n",
    "                                                 expand_binnumbers = True)\n",
    "    pickup_region = (binnum_pickup[0] - 1) * 50 + binnum_pickup[1]\n",
    "    \n",
    "    _, _, _, binnum_dropoff = binned_statistic_2d(data['dropoff_longitude'].to_numpy(),\n",
    "                                                 data['dropoff_latitude'].to_numpy(),\n",
    "                                                 values = None, statistic = 'count',\n",
    "                                                 bins = [50, 50],\n",
    "                                                 range = [[nyLongFrom, nyLongTo], [nyLatFrom, nyLatTo]],\n",
    "                                                 expand_binnumbers = True)\n",
    "    dropoff_region = (binnum_dropoff[0] - 1) * 50 + binnum_dropoff[1]\n",
    "    \n",
    "    data['pickup_region'] = pickup_region\n",
    "    data['dropoff_region'] = dropoff_region\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# доля каждого категориального признака в отдельный признак\n",
    "def feature_proportion(data, feature, new_colName):\n",
    "    feature_types = np.sort(data[feature].unique())\n",
    "    feature_dic = dict(zip(feature_types, ['{}_{}'.format(new_colName, i) for i in feature_types]))\n",
    "    \n",
    "    # группируем по региону, времени и <признаку>\n",
    "    groupped = data.groupby(by=['pickup_region', 'tpep_pickup_datetime', feature],\n",
    "                            as_index=False)[feature].agg(['count'])\n",
    "    \n",
    "    groupped = groupped.reset_index([feature])\n",
    "    \n",
    "    # меняем '1' на 'feature_1'\n",
    "    groupped[feature] = groupped[feature].map(feature_dic)\n",
    "\n",
    "    # кол-во поездок в каждой паре регион-час\n",
    "    counts = groupped.groupby(by=['pickup_region', 'tpep_pickup_datetime'])['count'].sum()\n",
    "\n",
    "    # создаем сводную\n",
    "    pivot = groupped.pivot_table(values='count', index=['pickup_region', 'tpep_pickup_datetime'],\n",
    "                                 columns=feature).fillna(0)\n",
    "    # делим на общее кол-во поездок в данный час - получаем долю типа оплаты\n",
    "    for col in pivot.columns[:-1]:\n",
    "        pivot[col] = pivot[col] / counts\n",
    "    \n",
    "    return pivot\n",
    "\n",
    "\n",
    "# преобразование и добавление признаков\n",
    "def transform_data(data_raw):    \n",
    "    # добавляем регионы посадки и высадки\n",
    "    data_raw = add_region(data_raw)\n",
    "    \n",
    "    # добавляем длительность поездки\n",
    "    data_raw['duration_sec'] = data_raw['tpep_dropoff_datetime'] - data_raw['tpep_pickup_datetime']\n",
    "    data_raw['duration_sec'] = data_raw['duration_sec'].dt.total_seconds()\n",
    "    \n",
    "    # обрезаем время начала и окончания поездки до часов\n",
    "    data_raw['tpep_pickup_datetime'] = data_raw['tpep_pickup_datetime'].dt.floor('H')\n",
    "    data_raw['tpep_dropoff_datetime'] = data_raw['tpep_dropoff_datetime'].dt.floor('H')\n",
    "    \n",
    "    # кол-во поездок в паре регион-час\n",
    "    data = data_raw.groupby(by=['pickup_region', 'tpep_pickup_datetime']).size().to_frame(name='count')\n",
    "    \n",
    "    # группируем признаки средних\n",
    "    columns_mean = ['duration_sec', 'passenger_count', 'trip_distance', 'total_amount']\n",
    "    data_means = data_raw.groupby(by=['tpep_pickup_datetime', 'pickup_region'])[columns_mean].mean()\n",
    "    data = data.join(data_means)\n",
    "    \n",
    "    # доли поездок по тарифам каждого типа\n",
    "    data_rate = feature_proportion(data_raw, 'ratecodeid', 'rate')\n",
    "    data = data.join(data_rate)\n",
    "    \n",
    "    # доли поездок по способам оплаты каждого типа\n",
    "    data_payment = feature_proportion(data_raw, 'payment_type', 'payment')\n",
    "    data = data.join(data_payment)\n",
    "    \n",
    "    # доли поездок по провайдерам каждого типа\n",
    "    data_provider = feature_proportion(data_raw, 'store_and_fwd_flag', 'provider')\n",
    "    data = data.join(data_provider)\n",
    "    \n",
    "    return data_raw, data\n",
    "\n",
    "\n",
    "\n",
    "# основной пайплайн обработки данных\n",
    "def load_transform_pipeline(file_list):\n",
    "    result_data, result_data_raw = None, None\n",
    "    \n",
    "    for i, file_path in enumerate(file_list):\n",
    "        print('file {} of {} processing'.format(i+1, len(file_list)))\n",
    "        \n",
    "        data = pd.read_csv(file_path, parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "        \n",
    "        data = clear_data(data)\n",
    "        \n",
    "        data_raw, data = transform_data(data)\n",
    "        \n",
    "        if result_data is None:\n",
    "            result_data_raw = data_raw\n",
    "            result_data = data\n",
    "        else:\n",
    "            result_data_raw = pd.concat([result_data_raw, data_raw])\n",
    "            result_data = pd.concat([result_data, data])\n",
    "        \n",
    "    result_data.reset_index('pickup_region', inplace=True)\n",
    "    return result_data_raw, result_data\n",
    "\n",
    "\n",
    "\n",
    "# добавлени признаков: год, меся, день, день недели, праздники,\n",
    "# кол-во поездок n-часов назад, предыдущие n-часов в текущем и соседних регионах\n",
    "def time_features(data, hours_back=[1, 6, 24, 48], rolling_back=[6, 24]):   \n",
    "    data['year'] = data.index.year\n",
    "    data['month'] = data.index.month\n",
    "    data['day'] = data.index.day\n",
    "    data['weekday'] = data.index.weekday\n",
    "    \n",
    "    for i in hours_back:\n",
    "        data['t-%d'%i] = data.iloc[:, 0].shift(i)\n",
    "    \n",
    "    for i in rolling_back:\n",
    "        data['prev_%dh'%i] = data.iloc[:, 0].rolling(i, i, closed='left').sum()\n",
    "        \n",
    "    for i in rolling_back:\n",
    "        data['prev_neighb_%dh'%i] = data.iloc[:, 1].rolling(i, i, closed='left').sum()\n",
    "    \n",
    "    for i in rolling_back:\n",
    "        data['prev_dropoff_perc_%dh'%i] = data.iloc[:, 2].rolling(i, i, closed='left').sum()\n",
    "    \n",
    "    data.drop(['count', 'neighbors_sum', 'dropoff_percent'], axis=1, inplace=True)\n",
    "    data.index.name = 'tpep_pickup_datetime'\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# гармоники Фурье\n",
    "def fourier_features(data, k=15):\n",
    "    for i in range(1,k+1):\n",
    "        data['sd%d'%i] = np.sin(np.arange(len(data)) * 2 * np.pi * i / 24)\n",
    "        data['cd%d'%i] = np.cos(np.arange(len(data)) * 2 * np.pi * i / 24)\n",
    "        data['sw%d'%i] = np.sin(np.arange(len(data)) * 2 * np.pi * i / 168)\n",
    "        data['cw%d'%i] = np.cos(np.arange(len(data)) * 2 * np.pi * i / 168)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# удаление высококоррелированных между собой признаков\n",
    "def hight_correlated_features(data, t):\n",
    "    corr = data.corr()\n",
    "    corrs = []\n",
    "    \n",
    "    for i, col in enumerate(corr.columns):\n",
    "        for j, row in enumerate(corr.index):\n",
    "            if j > i:\n",
    "                corrs.append([col, row, corr.loc[row, col]])\n",
    "    corrs = pd.DataFrame([[col,row,corr.loc[row, col],abs(corr.loc[row,col])]\n",
    "                          for i,col in enumerate(corr.columns)for j,row in enumerate(corr.index) if j > i])\n",
    "    corrs.columns = ['feat1', 'feat2', 'corr', 'abs_corr']\n",
    "    corrs.sort_values(by='abs_corr', ascending=False, inplace=True)\n",
    "    \n",
    "    to_drop_cols = corrs[corrs['abs_corr'] >= t]['feat2'].values\n",
    "    \n",
    "    return to_drop_cols\n",
    "\n",
    "\n",
    "\n",
    "# подбор коэф. альфа для Lasso\n",
    "def alpha_search(X_train, y_train, X_test, y_test, alphas, estimator, tol=1e-1, max_iter=1e3):\n",
    "    mae_ = []\n",
    "    for alpha in alphas:\n",
    "        estimator.alpha = alpha\n",
    "        estimator.tol=tol\n",
    "        estimator.max_iter=max_iter\n",
    "        estimator.fit(X_train, y_train)\n",
    "        pred = estimator.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, pred)\n",
    "        mae_.append(mae)\n",
    "\n",
    "    return alphas[np.array(mae_).argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a011c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_list = [[2016, i] for i in range(1, 7)]\n",
    "folder_path = r'/Users/salavat/OneDrive/3. Learn/ML_DA/COURSE6/Прогнозирование временных рядов/DATA/NYC_TLC/'\n",
    "file_list = [folder_path + f'yellow_tripdata_{year}-{month:02}.csv' for year, month in month_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d848ceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 1 of 6 processing\n",
      "file 2 of 6 processing\n",
      "file 3 of 6 processing\n",
      "file 4 of 6 processing\n",
      "file 5 of 6 processing\n",
      "file 6 of 6 processing\n"
     ]
    }
   ],
   "source": [
    "# загружаем и обрабатываем данные\n",
    "data_raw, data = load_transform_pipeline(file_list)\n",
    "\n",
    "data_raw.to_csv('data_raw.csv')\n",
    "data.to_csv('data.csv')\n",
    "\n",
    "data = pd.read_csv('data.csv', parse_dates=[0])\n",
    "data = data.set_index('tpep_pickup_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41883656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# составим сводную по количеству поездок в каждом регионе\n",
    "regions_count = data.pivot_table(values = 'count', index = 'tpep_pickup_datetime',\n",
    "                                 columns = 'pickup_region', fill_value = 0)\n",
    "regions_count.to_csv('regions_count.csv')\n",
    "\n",
    "regions_count = pd.read_csv('regions_count.csv', parse_dates=[0])\n",
    "regions_count = regions_count.set_index('tpep_pickup_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6940b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# посчитаем долю каждого региона в который совершались поездки\n",
    "dropoff_region_count = data_raw.groupby(['tpep_dropoff_datetime', 'dropoff_region']).size().to_frame(name='count')\n",
    "dropoff_region_pivot = dropoff_region_count.pivot_table(values = 'count',\n",
    "                                                        index = 'tpep_dropoff_datetime',\n",
    "                                                        columns = 'dropoff_region',\n",
    "                                                        fill_value=0)\n",
    "dropoff_region_percent = dropoff_region_pivot.apply(lambda x: x / sum(x) * 100, axis=1, result_type='expand')\n",
    "dropoff_region_percent[regions].head(2)\n",
    "del data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "546dcc07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# создадим отдельный датафрейм для каждого из 102 регионов\n",
    "regions_dfs = []\n",
    "\n",
    "for reg in regions:\n",
    "    df = data[data['pickup_region'] == reg].drop('pickup_region', axis=1)\n",
    "    df = pd.DataFrame(index = period_full).join(df)\n",
    "    df.index.name = 'tpep_pickup_datetime'\n",
    "    \n",
    "    # добавим долю региона в который совершалась поездка\n",
    "    df = df.join(dropoff_region_percent[[reg]].rename({reg: 'dropoff_percent'}, axis=1))\n",
    "    \n",
    "    # кол-во поездок в соседних регионах\n",
    "    neighbour_regions = list(set([reg-1, reg+1, reg-50, reg+50]) & set(regions_count))\n",
    "    df['neighbors_sum'] = regions_count[neighbour_regions].sum(axis=1)\n",
    "    \n",
    "    # временные признаки\n",
    "    cols = ['count', 'neighbors_sum', 'dropoff_percent']\n",
    "    df = df.join(time_features(df[cols].copy()))\n",
    "    \n",
    "    # удалим признаки cols характеризуемые текущим временем (кроме count)\n",
    "    df.drop(cols[1:], axis=1, inplace=True)\n",
    "    \n",
    "    # гармоники Фурье\n",
    "    df = fourier_features(df)\n",
    "    \n",
    "    # праздники\n",
    "    df['index_day'] = df.index.floor('D')\n",
    "    df = df.merge(holidays, how='left', left_on='index_day', right_index=True).fillna(0)    \n",
    "    df.drop('index_day', axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    # отмасштабируем признаки\n",
    "    scaler_train_period = pd.date_range(datetime(2015,3,1),datetime(2016,5,31,23), freq='H')\n",
    "    scaler = MinMaxScaler().fit(df.loc[period_full, df.columns[1:]])\n",
    "    df.iloc[:, 1:].values[:] = scaler.transform(df.loc[period_full, df.columns[1:]])\n",
    "    \n",
    "    # удалим сильно коррелированные между собой признаки\n",
    "    to_drop = hight_correlated_features(df.loc[period_train, df.columns[1:]], 0.95)\n",
    "    df.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    # сдвинем целевую переменную на [1..6]ч для каждой 6-ти моделей регрессий и удалим признак <count>\n",
    "    for i in range(1,7):\n",
    "        df['count_next_%dh'%i] = df['count'].shift(-i)\n",
    "    \n",
    "    regions_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "240c7a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка за май MAE: 24.059171980354666\n"
     ]
    }
   ],
   "source": [
    "# подберем параметр alpha для Lasso\n",
    "alpha_list = list(np.arange(0.1, 1, 0.1)) + list(range(1, 10))\n",
    "\n",
    "alphas = [] # лучшие альфа для каждого региона\n",
    "Q_may = [] # ошибки за май\n",
    "\n",
    "for reg_pos in range(102):\n",
    "    for i in range(1,7):\n",
    "        df = regions_dfs[reg_pos].copy()\n",
    "        X_train = df.loc[period_train, df.columns[1:-6]]\n",
    "        y_train = df.loc[period_train, 'count_next_%dh'%i]\n",
    "        \n",
    "        X_validation = df.loc[period_validation, df.columns[1:-6]]\n",
    "        y_validation = df.loc[period_validation, 'count_next_%dh'%i]\n",
    "        \n",
    "        best_alpha = alpha_search(X_train, y_train, X_validation, y_validation, alpha_list, Lasso())\n",
    "        alphas.append(best_alpha)\n",
    "        \n",
    "        model = Lasso(alpha=best_alpha).fit(X_train, y_train)\n",
    "        pred = model.predict(X_validation)\n",
    "        Q_may += [abs(err) for err in pred - y_validation]\n",
    "\n",
    "print('Ошибка за май MAE:', np.array(Q_may).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13c698da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка за июнь MAE: 22.772736957684018\n"
     ]
    }
   ],
   "source": [
    "Q_june = [] # ошибки за июнь\n",
    "models = [] # модели регрессии (102 региона x 6 моделей на каждый час прогноза)\n",
    "\n",
    "for reg_pos in range(102):\n",
    "    for i in range(1,7):\n",
    "        df = regions_dfs[reg_pos].copy()\n",
    "        X_train = df.loc[period_train2, df.columns[1:-6]]\n",
    "        y_train = df.loc[period_train2, 'count_next_%dh'%i]\n",
    "        \n",
    "        X_test = df.loc[period_test, df.columns[1:-6]]\n",
    "        y_test = df.loc[period_test, 'count_next_%dh'%i]\n",
    "        \n",
    "        model = Lasso(alpha=alphas[reg_pos], max_iter=1e4).fit(X_train, y_train)\n",
    "        models.append(model)\n",
    "        pred = model.predict(X_test)\n",
    "        Q_june += [abs(err) for err in pred - y_test]\n",
    "        \n",
    "        \n",
    "print('Ошибка за июнь MAE:', np.mean(Q_june))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4154921e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>abs_coef</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sd2</th>\n",
       "      <td>3.995338</td>\n",
       "      <td>3.995338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sd3</th>\n",
       "      <td>3.096218</td>\n",
       "      <td>3.096218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sd1</th>\n",
       "      <td>2.618178</td>\n",
       "      <td>2.618178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sd4</th>\n",
       "      <td>2.152269</td>\n",
       "      <td>2.152269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cd5</th>\n",
       "      <td>-1.806796</td>\n",
       "      <td>1.806796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cd2</th>\n",
       "      <td>1.275179</td>\n",
       "      <td>1.275179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sw8</th>\n",
       "      <td>-1.192878</td>\n",
       "      <td>1.192878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cw15</th>\n",
       "      <td>0.864279</td>\n",
       "      <td>0.864279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sw13</th>\n",
       "      <td>0.744913</td>\n",
       "      <td>0.744913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cd3</th>\n",
       "      <td>-0.726560</td>\n",
       "      <td>0.726560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             coef  abs_coef\n",
       "feature                    \n",
       "sd2      3.995338  3.995338\n",
       "sd3      3.096218  3.096218\n",
       "sd1      2.618178  2.618178\n",
       "sd4      2.152269  2.152269\n",
       "cd5     -1.806796  1.806796\n",
       "cd2      1.275179  1.275179\n",
       "sw8     -1.192878  1.192878\n",
       "cw15     0.864279  0.864279\n",
       "sw13     0.744913  0.744913\n",
       "cd3     -0.726560  0.726560"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим на коэфиценты регрессии одного из регионов\n",
    "coef_df = pd.DataFrame(models[70*7].coef_, index = regions_dfs[70].columns[1:-6], columns = ['coef'])\n",
    "coef_df.index.name = 'feature'\n",
    "coef_df['abs_coef'] = coef_df['coef'].apply(abs)\n",
    "coef_df.sort_values(by='abs_coef', ascending=False, inplace=True)\n",
    "coef_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a9c76",
   "metadata": {},
   "source": [
    "### Видно, что наибольшие коэфиценты получили признаки гармоник Фурье.<br>Дополнительные признаки не сильно помогли.<br>Думаю улучить результат помог бы учет годовой сезонности."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
